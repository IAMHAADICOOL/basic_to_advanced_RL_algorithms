{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67rhCAvE7vV4",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "<div style=\"text-align:center\">\n",
        "    <h1>\n",
        "        Advantage Actor-Critic (A2C)\n",
        "    </h1>\n",
        "</div>\n",
        "\n",
        "<br><br>\n",
        "\n",
        "<div style=\"text-align:center\">\n",
        "In this notebook we are going to combine temporal difference learning (TD) with policy gradient methods. The resulting algorithm is called Advantage Actor-Critic (A2C) and uses a one-step estimate of the return to update the policy:\n",
        "</div>\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat G_t = R_{t+1} + \\gamma v(S_{t+1}|w)\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "_uUI8vNy703R"
      },
      "outputs": [],
      "source": [
        "# @title Setup code (not important) - Run this cell by pressing \"Shift + Enter\"\n",
        "\n",
        "\n",
        "\n",
        "# !pip install -qq gym==0.23.0\n",
        "\n",
        "\n",
        "from typing import Tuple, Dict, Optional, Iterable, Callable\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import torch\n",
        "from matplotlib import animation\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.error import DependencyNotInstalled\n",
        "\n",
        "import pygame\n",
        "from pygame import gfxdraw\n",
        "\n",
        "\n",
        "class Maze(gym.Env):\n",
        "\n",
        "    def __init__(self, exploring_starts: bool = False,\n",
        "                 shaped_rewards: bool = False, size: int = 5) -> None:\n",
        "        super().__init__()\n",
        "        self.exploring_starts = exploring_starts\n",
        "        self.shaped_rewards = shaped_rewards\n",
        "        self.state = (size - 1, size - 1)\n",
        "        self.goal = (size - 1, size - 1)\n",
        "        self.maze = self._create_maze(size=size)\n",
        "        self.distances = self._compute_distances(self.goal, self.maze)\n",
        "        self.action_space = spaces.Discrete(n=4)\n",
        "        self.action_space.action_meanings = {0: 'UP', 1: 'RIGHT', 2: 'DOWN', 3: \"LEFT\"}\n",
        "        self.observation_space = spaces.MultiDiscrete([size, size])\n",
        "\n",
        "        self.screen = None\n",
        "        self.agent_transform = None\n",
        "\n",
        "    def step(self, action: int) -> Tuple[Tuple[int, int], float, bool, Dict]:\n",
        "        reward = self.compute_reward(self.state, action)\n",
        "        self.state = self._get_next_state(self.state, action)\n",
        "        done = self.state == self.goal\n",
        "        info = {}\n",
        "        return self.state, reward, done, info\n",
        "\n",
        "    def reset(self) -> Tuple[int, int]:\n",
        "        if self.exploring_starts:\n",
        "            while self.state == self.goal:\n",
        "                self.state = tuple(self.observation_space.sample())\n",
        "        else:\n",
        "            self.state = (0, 0)\n",
        "        return self.state\n",
        "\n",
        "    def render(self, mode: str = 'human') -> Optional[np.ndarray]:\n",
        "        assert mode in ['human', 'rgb_array']\n",
        "\n",
        "        screen_size = 600\n",
        "        scale = screen_size / 5\n",
        "\n",
        "        if self.screen is None:\n",
        "            pygame.init()\n",
        "            self.screen = pygame.Surface((screen_size, screen_size))\n",
        "\n",
        "        surf = pygame.Surface((screen_size, screen_size))\n",
        "        surf.fill((22, 36, 71))\n",
        "\n",
        "\n",
        "        for row in range(5):\n",
        "            for col in range(5):\n",
        "\n",
        "                state = (row, col)\n",
        "                for next_state in [(row + 1, col), (row - 1, col), (row, col + 1), (row, col - 1)]:\n",
        "                    if next_state not in self.maze[state]:\n",
        "\n",
        "                        # Add the geometry of the edges and walls (i.e. the boundaries between\n",
        "                        # adjacent squares that are not connected).\n",
        "                        row_diff, col_diff = np.subtract(next_state, state)\n",
        "                        left = (col + (col_diff > 0)) * scale - 2 * (col_diff != 0)\n",
        "                        right = ((col + 1) - (col_diff < 0)) * scale + 2 * (col_diff != 0)\n",
        "                        top = (5 - (row + (row_diff > 0))) * scale - 2 * (row_diff != 0)\n",
        "                        bottom = (5 - ((row + 1) - (row_diff < 0))) * scale + 2 * (row_diff != 0)\n",
        "\n",
        "                        gfxdraw.filled_polygon(surf, [(left, bottom), (left, top), (right, top), (right, bottom)], (255, 255, 255))\n",
        "\n",
        "        # Add the geometry of the goal square to the viewer.\n",
        "        left, right, top, bottom = scale * 4 + 10, scale * 5 - 10, scale - 10, 10\n",
        "        gfxdraw.filled_polygon(surf, [(left, bottom), (left, top), (right, top), (right, bottom)], (40, 199, 172))\n",
        "\n",
        "        # Add the geometry of the agent to the viewer.\n",
        "        agent_row = int(screen_size - scale * (self.state[0] + .5))\n",
        "        agent_col = int(scale * (self.state[1] + .5))\n",
        "        gfxdraw.filled_circle(surf, agent_col, agent_row, int(scale * .6 / 2), (228, 63, 90))\n",
        "\n",
        "        surf = pygame.transform.flip(surf, False, True)\n",
        "        self.screen.blit(surf, (0, 0))\n",
        "\n",
        "        return np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
        "            )\n",
        "\n",
        "    def close(self) -> None:\n",
        "        if self.screen is not None:\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()\n",
        "            self.screen = None\n",
        "\n",
        "    def compute_reward(self, state: Tuple[int, int], action: int) -> float:\n",
        "        next_state = self._get_next_state(state, action)\n",
        "        if self.shaped_rewards:\n",
        "            return - (self.distances[next_state] / self.distances.max())\n",
        "        return - float(state != self.goal)\n",
        "\n",
        "    def simulate_step(self, state: Tuple[int, int], action: int):\n",
        "        reward = self.compute_reward(state, action)\n",
        "        next_state = self._get_next_state(state, action)\n",
        "        done = next_state == self.goal\n",
        "        info = {}\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "    def _get_next_state(self, state: Tuple[int, int], action: int) -> Tuple[int, int]:\n",
        "        if action == 0:\n",
        "            next_state = (state[0] - 1, state[1])\n",
        "        elif action == 1:\n",
        "            next_state = (state[0], state[1] + 1)\n",
        "        elif action == 2:\n",
        "            next_state = (state[0] + 1, state[1])\n",
        "        elif action == 3:\n",
        "            next_state = (state[0], state[1] - 1)\n",
        "        else:\n",
        "            raise ValueError(\"Action value not supported:\", action)\n",
        "        if next_state in self.maze[state]:\n",
        "            return next_state\n",
        "        return state\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_maze(size: int) -> Dict[Tuple[int, int], Iterable[Tuple[int, int]]]:\n",
        "        maze = {(row, col): [(row - 1, col), (row + 1, col), (row, col - 1), (row, col + 1)]\n",
        "                for row in range(size) for col in range(size)}\n",
        "\n",
        "        left_edges = [[(row, 0), (row, -1)] for row in range(size)]\n",
        "        right_edges = [[(row, size - 1), (row, size)] for row in range(size)]\n",
        "        upper_edges = [[(0, col), (-1, col)] for col in range(size)]\n",
        "        lower_edges = [[(size - 1, col), (size, col)] for col in range(size)]\n",
        "        walls = [\n",
        "            [(1, 0), (1, 1)], [(2, 0), (2, 1)], [(3, 0), (3, 1)],\n",
        "            [(1, 1), (1, 2)], [(2, 1), (2, 2)], [(3, 1), (3, 2)],\n",
        "            [(3, 1), (4, 1)], [(0, 2), (1, 2)], [(1, 2), (1, 3)],\n",
        "            [(2, 2), (3, 2)], [(2, 3), (3, 3)], [(2, 4), (3, 4)],\n",
        "            [(4, 2), (4, 3)], [(1, 3), (1, 4)], [(2, 3), (2, 4)],\n",
        "        ]\n",
        "\n",
        "        obstacles = upper_edges + lower_edges + left_edges + right_edges + walls\n",
        "\n",
        "        for src, dst in obstacles:\n",
        "            maze[src].remove(dst)\n",
        "\n",
        "            if dst in maze:\n",
        "                maze[dst].remove(src)\n",
        "\n",
        "        return maze\n",
        "\n",
        "    @staticmethod\n",
        "    def _compute_distances(goal: Tuple[int, int],\n",
        "                           maze: Dict[Tuple[int, int], Iterable[Tuple[int, int]]]) -> np.ndarray:\n",
        "        distances = np.full((5, 5), np.inf)\n",
        "        visited = set()\n",
        "        distances[goal] = 0.\n",
        "\n",
        "        while visited != set(maze):\n",
        "            sorted_dst = [(v // 5, v % 5) for v in distances.argsort(axis=None)]\n",
        "            closest = next(x for x in sorted_dst if x not in visited)\n",
        "            visited.add(closest)\n",
        "\n",
        "            for neighbour in maze[closest]:\n",
        "                distances[neighbour] = min(distances[neighbour], distances[closest] + 1)\n",
        "        return distances\n",
        "\n",
        "\n",
        "def display_video(frames):\n",
        "    # Copied from: https://colab.research.google.com/github/deepmind/dm_control/blob/master/tutorial.ipynb\n",
        "    orig_backend = matplotlib.get_backend()\n",
        "    matplotlib.use('Agg')\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
        "    matplotlib.use(orig_backend)\n",
        "    ax.set_axis_off()\n",
        "    ax.set_aspect('equal')\n",
        "    ax.set_position([0, 0, 1, 1])\n",
        "    im = ax.imshow(frames[0])\n",
        "    def update(frame):\n",
        "        im.set_data(frame)\n",
        "        return [im]\n",
        "    anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
        "                                    interval=50, blit=True, repeat=False)\n",
        "    return HTML(anim.to_html5_video())\n",
        "\n",
        "\n",
        "def seed_everything(env: gym.Env, seed: int = 42) -> None:\n",
        "    env.seed(seed)\n",
        "    env.action_space.seed(seed)\n",
        "    env.observation_space.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.use_deterministic_algorithms(True)\n",
        "\n",
        "\n",
        "def plot_stats(stats):\n",
        "    rows = len(stats)\n",
        "    cols = 1\n",
        "\n",
        "    fig, ax = plt.subplots(rows, cols, figsize=(12, 6))\n",
        "\n",
        "    for i, key in enumerate(stats):\n",
        "        vals = stats[key]\n",
        "        vals = [np.mean(vals[i-10:i+10]) for i in range(10, len(vals)-10)]\n",
        "        if len(stats) > 1:\n",
        "            ax[i].plot(range(len(vals)), vals)\n",
        "            ax[i].set_title(key, size=18)\n",
        "        else:\n",
        "            ax.plot(range(len(vals)), vals)\n",
        "            ax.set_title(key, size=18)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def test_policy_network(env, policy, episodes=10):\n",
        "    frames = []\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        frames.append(env.render(mode=\"rgb_array\"))\n",
        "\n",
        "        while not done:\n",
        "            state = torch.from_numpy(state).unsqueeze(0).float()\n",
        "            action = policy(state).multinomial(1).item()\n",
        "            next_state, _, done, _ = env.step(action)\n",
        "            img = env.render(mode=\"rgb_array\")\n",
        "            frames.append(img)\n",
        "            state = next_state\n",
        "\n",
        "    return display_video(frames)\n",
        "\n",
        "\n",
        "def plot_action_probs(probs, labels):\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.bar(labels, probs, color ='orange')\n",
        "    plt.title(\"$\\pi(s)$\", size=16)\n",
        "    plt.xticks(fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlB0Tbp07vV6"
      },
      "source": [
        "## Import the necessary software libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2OnbUU8t7vV7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from torch import nn as nn\n",
        "from torch.optim import AdamW\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPEwlOrt7vV8"
      },
      "source": [
        "## Create and preprocess the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j37j_pOh7vV8"
      },
      "source": [
        "### Create the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RDViC8L47vV8"
      },
      "outputs": [],
      "source": [
        "env = gym.make('Acrobot-v1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LuJ9Hx4E7vV8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "State dimensions: 6. Actions: 3\n",
            "Sample state: [ 0.9976372  -0.0687022   0.99747175  0.07106436  0.09952347  0.08594623]\n"
          ]
        }
      ],
      "source": [
        "dims = env.observation_space.shape[0]\n",
        "actions = env.action_space.n\n",
        "\n",
        "print(f\"State dimensions: {dims}. Actions: {actions}\")\n",
        "print(f\"Sample state: {env.reset()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QVHSTC827vV8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x1a2bffca550>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(env.render(mode='rgb_array'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8piDIP4E7vV9"
      },
      "source": [
        "### Prepare the environment to work with PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bBqxt5hP7vV9"
      },
      "outputs": [],
      "source": [
        "class PreprocessEnv(gym.Wrapper):\n",
        "\n",
        "    def __init__(self, env):\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "\n",
        "    def reset(self):\n",
        "        state = self.env.reset()\n",
        "        return torch.from_numpy(state).float()\n",
        "\n",
        "    def step(self, actions):\n",
        "        actions = actions.squeeze().numpy()\n",
        "        next_state, reward, done, info = self.env.step(actions)\n",
        "        next_state = torch.from_numpy(next_state).float()\n",
        "        reward = torch.tensor(reward).unsqueeze(1).float()\n",
        "        done = torch.tensor(done).unsqueeze(1)\n",
        "        return next_state, reward, done, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jkRkODiA7vV-"
      },
      "outputs": [],
      "source": [
        "num_envs = 8\n",
        "parallel_env = gym.vector.make('Acrobot-v1', num_envs=num_envs)\n",
        "seed_everything(parallel_env)\n",
        "parallel_env = PreprocessEnv(parallel_env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_35Rcxy7vV-"
      },
      "source": [
        "### Create the policy $\\pi(s)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rF1_C5Xb7vV-"
      },
      "outputs": [],
      "source": [
        "actor = nn.Sequential(\n",
        "    nn.Linear(dims, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, actions),\n",
        "    nn.Softmax(dim=-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPY8NEB17vV-"
      },
      "source": [
        "### Create the value network $v(s)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EKD6vk4i7vV-"
      },
      "outputs": [],
      "source": [
        "critic = nn.Sequential(\n",
        "    nn.Linear(dims, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH5RVyq-7vV-"
      },
      "source": [
        "## Implement the algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lI3Zju7u7vV-"
      },
      "outputs": [],
      "source": [
        "def actor_critic(actor, critic, episodes, alpha=1e-4, gamma=0.99):\n",
        "    actor_optim = AdamW(actor.parameters(), lr=1e-3)\n",
        "    critic_optim = AdamW(critic.parameters(), lr=1e-4)\n",
        "    stats = {'Actor Loss': [], 'Critic Loss': [], 'Returns': []}\n",
        "\n",
        "    for episode in tqdm(range(1, episodes + 1)):\n",
        "        state = parallel_env.reset()\n",
        "        done_b = torch.zeros((num_envs, 1), dtype=torch.bool)\n",
        "        ep_return = torch.zeros((num_envs, 1))\n",
        "        I = 1.\n",
        "\n",
        "        while not done_b.all():\n",
        "            action = actor(state).multinomial(1).detach()\n",
        "            next_state, reward, done, _ = parallel_env.step(action)\n",
        "\n",
        "            value = critic(state)\n",
        "            target = reward + ~done * gamma * critic(next_state).detach()\n",
        "            critic_loss = F.mse_loss(value, target)\n",
        "            critic.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            critic_optim.step()\n",
        "\n",
        "            advantage = (target - value).detach()\n",
        "            probs = actor(state)\n",
        "            log_probs = torch.log(probs + 1e-6)\n",
        "            action_log_prob = log_probs.gather(1, action)\n",
        "            entropy = - torch.sum(probs * log_probs, dim=-1, keepdim=True)\n",
        "            actor_loss = - I * action_log_prob * advantage - 0.01 * entropy\n",
        "            actor_loss = actor_loss.mean()\n",
        "            actor.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            actor_optim.step()\n",
        "\n",
        "            ep_return += reward\n",
        "            done_b |= done\n",
        "            state = next_state\n",
        "            I = I * gamma\n",
        "\n",
        "        stats['Actor Loss'].append(actor_loss.item())\n",
        "        stats['Critic Loss'].append(critic_loss.item())\n",
        "        stats['Returns'].append(ep_return.mean().item())\n",
        "\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eEWU63Z07vV-",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [05:51<00:00,  3.51s/it]\n"
          ]
        }
      ],
      "source": [
        "stats = actor_critic(actor, critic, 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoxzCbPz7vV-"
      },
      "source": [
        "## Show results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cmyUHP67vV-"
      },
      "source": [
        "### Show execution stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oV46xCdU7vV-"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'plot_stats' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplot_stats\u001b[49m(stats)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'plot_stats' is not defined"
          ]
        }
      ],
      "source": [
        "plot_stats(stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD1Khhk17vV-"
      },
      "source": [
        "### Test the resulting agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6EwEhPd7vV_"
      },
      "outputs": [],
      "source": [
        "test_policy_network(env, actor, episodes=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHS56xgc7vV_"
      },
      "source": [
        "## Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk1oi1-_7vV_"
      },
      "source": [
        "[[1] Reinforcement Learning: An Introduction. Ch.13](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
